{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16e71784-b964-42ac-ae44-66922d03a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from cerebras.cloud.sdk import Cerebras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efea6195-fba9-4ed6-a555-20a2ca0b8073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6d7ff49-a036-4891-9472-2cad09d8c671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast inference is crucial in various fields, such as:\n",
      "\n",
      "1. **Real-time systems**: In applications like self-driving cars, medical diagnosis, or video analysis, fast inference enables timely decision-making and responses to changing situations. This reduces latency, ensures safety, and improves the overall efficiency of the system.\n",
      "\n",
      "2. **Scalability**: As models become increasingly complex and large-scale, fast inference is necessary for parallel processing and distributed computing. This scalability allows for faster processing of larger models, enabling the analysis of vast amounts of data and the generation of high-quality results.\n",
      "\n",
      "3. **Edge computing**: With the growth of edge computing and the Internet of Things (IoT), devices require fast inference to process data locally and reduce the need for centralized processing. This enables faster decision-making, reduces latency, and improves energy efficiency.\n",
      "\n",
      "4. **Energy efficiency**: In resource-constrained devices or battery-powered applications, fast inference is crucial to minimize energy consumption and prolong battery life. By reducing inference times, devices can maintain optimal performance while conserving energy.\n",
      "\n",
      "5. **User experience**: In applications like language translation, image recognition, or sentiment analysis, fast inference provides a seamless user experience by delivering timely and accurate results. This enhances user satisfaction, engagement, and overall product experience.\n",
      "\n",
      "6. **Cost-effectiveness**: As computing resources become increasingly expensive, fast inference enables the efficient use of resources, reducing the need for additional hardware or computational capacity. This cost-effectiveness makes models more affordable and accessible to a wider range of users.\n",
      "\n",
      "By prioritizing fast inference, developers can create more efficient, scalable, and user-friendly systems that meet the demands of modern applications and users.\n"
     ]
    }
   ],
   "source": [
    "client = Cerebras(\n",
    "  api_key=os.environ.get(\"CEREBRAS_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "  messages=[\n",
    "  {\"role\": \"user\", \"content\": \"Why is fast inference important?\",}\n",
    "],\n",
    "  model=\"llama3.1-8b\",\n",
    ")\n",
    "\n",
    "message = chat_completion.choices[0].message.content\n",
    "print(message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
